#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„œë¹„ìŠ¤ ë°ëª¨
ë‹¤ì–‘í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ì²˜ë¦¬ ì„œë¹„ìŠ¤ë¥¼ ì½˜ì†”ì—ì„œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import cv2
import numpy as np
from ultralytics import YOLO
import mediapipe as mp
import argparse
import os
from pathlib import Path
import json

# PyTorch 2.6 í˜¸í™˜ì„± ì„¤ì •
import torch
torch.serialization.add_safe_globals(['ultralytics.nn.tasks.SegmentationModel'])

class SegmentationServicesDemo:
    def __init__(self):
        """ì„œë¹„ìŠ¤ ë°ëª¨ ì´ˆê¸°í™”"""
        print("ğŸ”§ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        # YOLO ëª¨ë¸ ë¡œë“œ
        self.yolo_model = YOLO('yolov8n-seg.pt')
        
        # MediaPipe ì–¼êµ´ ëœë“œë§ˆí¬
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=True,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5
        )
        
        # ê²°ê³¼ í´ë” ìƒì„±
        self.results_dir = Path('results')
        self.results_dir.mkdir(exist_ok=True)
        
        print("âœ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")

    def load_image(self, image_path):
        """ì´ë¯¸ì§€ ë¡œë“œ"""
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        return image

    def save_result(self, image, filename):
        """ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥"""
        result_path = self.results_dir / filename
        cv2.imwrite(str(result_path), image)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {result_path}")
        return str(result_path)

    def demo_background_removal(self, image_path, target_class='person'):
        """ë°°ê²½ ì œê±° ë°ëª¨"""
        print(f"\nğŸ¯ ë°°ê²½ ì œê±° ì„œë¹„ìŠ¤ ë°ëª¨")
        print(f"ğŸ“ ì´ë¯¸ì§€: {image_path}")
        print(f"ğŸ¯ ëŒ€ìƒ ê°ì²´: {target_class}")
        
        image = self.load_image(image_path)
        
        # ê°ì²´ ê°ì§€
        results = self.yolo_model(image)
        
        # ë§ˆìŠ¤í¬ ìƒì„±
        mask = np.zeros(image.shape[:2], dtype=np.uint8)
        
        for result in results:
            boxes = result.boxes
            masks = result.masks
            
            if boxes is not None and masks is not None:
                for i, box in enumerate(boxes):
                    cls = int(box.cls[0].cpu().numpy())
                    class_name = self.yolo_model.names[cls]
                    
                    if class_name == target_class:
                        mask_data = masks.data[i].cpu().numpy()
                        mask_resized = cv2.resize(mask_data, (image.shape[1], image.shape[0]))
                        mask = np.maximum(mask, (mask_resized > 0.5).astype(np.uint8) * 255)
        
        # ë°°ê²½ ì œê±°
        result_image = image.copy()
        result_image[mask == 0] = [255, 255, 255]  # í°ìƒ‰ ë°°ê²½
        
        # ê²°ê³¼ ì €ì¥
        filename = f"bg_removed_{target_class}.png"
        self.save_result(result_image, filename)
        
        print("âœ… ë°°ê²½ ì œê±° ì™„ë£Œ!")
        return result_image

    def demo_object_extraction(self, image_path, target_classes=None):
        """ê°ì²´ ì¶”ì¶œ ë°ëª¨"""
        print(f"\nâœ‚ï¸ ê°ì²´ ì¶”ì¶œ ì„œë¹„ìŠ¤ ë°ëª¨")
        print(f"ğŸ“ ì´ë¯¸ì§€: {image_path}")
        print(f"ğŸ¯ ëŒ€ìƒ ê°ì²´: {target_classes or 'ëª¨ë“  ê°ì²´'}")
        
        image = self.load_image(image_path)
        results = self.yolo_model(image)
        extracted_objects = []
        
        for result in results:
            boxes = result.boxes
            masks = result.masks
            
            if boxes is not None and masks is not None:
                for i, box in enumerate(boxes):
                    cls = int(box.cls[0].cpu().numpy())
                    class_name = self.yolo_model.names[cls]
                    
                    if target_classes is None or class_name in target_classes:
                        # ë§ˆìŠ¤í¬ ì ìš©
                        mask_data = masks.data[i].cpu().numpy()
                        mask_resized = cv2.resize(mask_data, (image.shape[1], image.shape[0]))
                        mask_bool = mask_resized > 0.5
                        
                        # ê°ì²´ ì¶”ì¶œ
                        extracted = image.copy()
                        extracted[~mask_bool] = [255, 255, 255]
                        
                        # ê²°ê³¼ ì €ì¥
                        filename = f"extracted_{class_name}_{len(extracted_objects)}.png"
                        self.save_result(extracted, filename)
                        
                        extracted_objects.append({
                            'class': class_name,
                            'filename': filename,
                            'confidence': float(box.conf[0].cpu().numpy())
                        })
        
        print(f"âœ… {len(extracted_objects)}ê°œ ê°ì²´ ì¶”ì¶œ ì™„ë£Œ!")
        for obj in extracted_objects:
            print(f"  - {obj['class']} (ì‹ ë¢°ë„: {obj['confidence']:.2f})")
        
        return extracted_objects

    def demo_face_beauty(self, image_path):
        """ì–¼êµ´ ë·°í‹° ë°ëª¨"""
        print(f"\nğŸ’„ ì–¼êµ´ ë·°í‹° ì„œë¹„ìŠ¤ ë°ëª¨")
        print(f"ğŸ“ ì´ë¯¸ì§€: {image_path}")
        
        image = self.load_image(image_path)
        
        # ì–¼êµ´ ëœë“œë§ˆí¬ ê²€ì¶œ
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_image)
        
        result_image = image.copy()
        
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                h, w, _ = image.shape
                
                # ì–¼êµ´ ì˜ì—­ ë§ˆìŠ¤í¬ ìƒì„±
                face_mask = np.zeros((h, w), dtype=np.uint8)
                
                # ì–¼êµ´ ìœ¤ê³½ ëœë“œë§ˆí¬
                face_oval_indices = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,
                                   397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,
                                   172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67, 109, 10]
                
                face_points = []
                for idx in face_oval_indices:
                    if idx < len(face_landmarks.landmark):
                        landmark = face_landmarks.landmark[idx]
                        x = int(landmark.x * w)
                        y = int(landmark.y * h)
                        face_points.append([x, y])
                
                if len(face_points) > 2:
                    # ì–¼êµ´ ë§ˆìŠ¤í¬ ìƒì„±
                    face_points = np.array(face_points, dtype=np.int32)
                    cv2.fillPoly(face_mask, [face_points], 255)
                    
                    # ë·°í‹° í•„í„° ì ìš©
                    # 1. ìŠ¤ë¬´ë”© (ë¸”ëŸ¬)
                    smoothed = cv2.GaussianBlur(result_image, (5, 5), 0)
                    result_image = np.where(face_mask[:, :, np.newaxis] > 0, 
                                          smoothed, result_image)
                    
                    # 2. ë°ê¸° ì¡°ì •
                    hsv = cv2.cvtColor(result_image, cv2.COLOR_BGR2HSV)
                    hsv[face_mask > 0, 2] = np.clip(hsv[face_mask > 0, 2] * 1.1, 0, 255)
                    result_image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
            
            # ê²°ê³¼ ì €ì¥
            filename = "face_beauty.jpg"
            self.save_result(result_image, filename)
            print("âœ… ì–¼êµ´ ë·°í‹° í•„í„° ì ìš© ì™„ë£Œ!")
        else:
            print("âš ï¸ ì–¼êµ´ì„ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return result_image

    def demo_object_counting(self, image_path, target_classes=None):
        """ê°ì²´ ì¹´ìš´íŒ… ë°ëª¨"""
        print(f"\nğŸ”¢ ê°ì²´ ì¹´ìš´íŒ… ì„œë¹„ìŠ¤ ë°ëª¨")
        print(f"ğŸ“ ì´ë¯¸ì§€: {image_path}")
        print(f"ğŸ¯ ëŒ€ìƒ ê°ì²´: {target_classes or 'ëª¨ë“  ê°ì²´'}")
        
        image = self.load_image(image_path)
        results = self.yolo_model(image)
        object_counts = {}
        object_details = []
        
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    cls = int(box.cls[0].cpu().numpy())
                    class_name = self.yolo_model.names[cls]
                    confidence = float(box.conf[0].cpu().numpy())
                    bbox = box.xyxy[0].cpu().numpy().tolist()
                    
                    if target_classes is None or class_name in target_classes:
                        object_counts[class_name] = object_counts.get(class_name, 0) + 1
                        object_details.append({
                            'class': class_name,
                            'confidence': confidence,
                            'bbox': bbox
                        })
        
        print(f"âœ… ê°ì²´ ì¹´ìš´íŒ… ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ê°ì²´ ìˆ˜: {sum(object_counts.values())}ê°œ")
        for obj, count in object_counts.items():
            print(f"  - {obj}: {count}ê°œ")
        
        return object_counts, object_details

    def demo_image_analysis(self, image_path):
        """ì´ë¯¸ì§€ ë¶„ì„ ë°ëª¨"""
        print(f"\nğŸ“Š ì´ë¯¸ì§€ ë¶„ì„ ì„œë¹„ìŠ¤ ë°ëª¨")
        print(f"ğŸ“ ì´ë¯¸ì§€: {image_path}")
        
        image = self.load_image(image_path)
        results = self.yolo_model(image)
        
        analysis = {
            'objects': [],
            'total_objects': 0,
            'image_size': image.shape,
            'dominant_objects': [],
            'object_counts': {}
        }
        
        object_counts = {}
        
        for result in results:
            boxes = result.boxes
            masks = result.masks
            
            if boxes is not None:
                for i, box in enumerate(boxes):
                    cls = int(box.cls[0].cpu().numpy())
                    class_name = self.yolo_model.names[cls]
                    conf = float(box.conf[0].cpu().numpy())
                    bbox = box.xyxy[0].cpu().numpy().tolist()
                    
                    # ê°ì²´ ì •ë³´
                    obj_info = {
                        'class': class_name,
                        'confidence': conf,
                        'bbox': bbox,
                        'area': (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
                    }
                    
                    # ë§ˆìŠ¤í¬ ì •ë³´ ì¶”ê°€
                    if masks is not None and i < len(masks.data):
                        mask_data = masks.data[i].cpu().numpy()
                        mask_resized = cv2.resize(mask_data, (image.shape[1], image.shape[0]))
                        mask_area = np.sum(mask_resized > 0.5)
                        obj_info['mask_area'] = int(mask_area)
                    
                    analysis['objects'].append(obj_info)
                    object_counts[class_name] = object_counts.get(class_name, 0) + 1
        
        analysis['total_objects'] = len(analysis['objects'])
        analysis['object_counts'] = object_counts
        
        # ì£¼ìš” ê°ì²´ ì°¾ê¸° (ë©´ì  ê¸°ì¤€)
        if analysis['objects']:
            sorted_objects = sorted(analysis['objects'], key=lambda x: x.get('mask_area', 0), reverse=True)
            analysis['dominant_objects'] = [obj['class'] for obj in sorted_objects[:3]]
        
        print("âœ… ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {analysis['image_size'][1]}x{analysis['image_size'][0]}")
        print(f"ğŸ¯ ì´ ê°ì²´ ìˆ˜: {analysis['total_objects']}ê°œ")
        print(f"ğŸ† ì£¼ìš” ê°ì²´: {', '.join(analysis['dominant_objects'])}")
        print(f"ğŸ“Š ê°ì²´ ë¶„í¬:")
        for obj, count in analysis['object_counts'].items():
            print(f"  - {obj}: {count}ê°œ")
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        analysis_file = self.results_dir / "image_analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {analysis_file}")
        
        return analysis

    def demo_image_filter(self, image_path, filter_type='blur', target_class='person'):
        """ì´ë¯¸ì§€ í•„í„° ë°ëª¨"""
        print(f"\nğŸ¨ ì´ë¯¸ì§€ í•„í„° ì„œë¹„ìŠ¤ ë°ëª¨")
        print(f"ğŸ“ ì´ë¯¸ì§€: {image_path}")
        print(f"ğŸ¨ í•„í„°: {filter_type}")
        print(f"ğŸ¯ ëŒ€ìƒ ê°ì²´: {target_class}")
        
        image = self.load_image(image_path)
        results = self.yolo_model(image)
        result_image = image.copy()
        
        for result in results:
            boxes = result.boxes
            masks = result.masks
            
            if boxes is not None and masks is not None:
                for i, box in enumerate(boxes):
                    cls = int(box.cls[0].cpu().numpy())
                    class_name = self.yolo_model.names[cls]
                    
                    if class_name == target_class:
                        mask_data = masks.data[i].cpu().numpy()
                        mask_resized = cv2.resize(mask_data, (image.shape[1], image.shape[0]))
                        mask_bool = mask_resized > 0.5
                        
                        # í•„í„° ì ìš©
                        if filter_type == 'blur':
                            blurred = cv2.GaussianBlur(result_image, (15, 15), 0)
                            result_image[mask_bool] = blurred[mask_bool]
                        elif filter_type == 'brightness':
                            hsv = cv2.cvtColor(result_image, cv2.COLOR_BGR2HSV)
                            hsv[mask_bool, 2] = np.clip(hsv[mask_bool, 2] * 1.5, 0, 255)
                            result_image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
                        elif filter_type == 'contrast':
                            lab = cv2.cvtColor(result_image, cv2.COLOR_BGR2LAB)
                            lab[mask_bool, 0] = np.clip(lab[mask_bool, 0] * 1.3, 0, 255)
                            result_image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        
        # ê²°ê³¼ ì €ì¥
        filename = f"filtered_{filter_type}_{target_class}.jpg"
        self.save_result(result_image, filename)
        
        print("âœ… ì´ë¯¸ì§€ í•„í„° ì ìš© ì™„ë£Œ!")
        return result_image

    def run_all_demos(self, image_path):
        """ëª¨ë“  ì„œë¹„ìŠ¤ ë°ëª¨ ì‹¤í–‰"""
        print("ğŸš€ ëª¨ë“  ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„œë¹„ìŠ¤ ë°ëª¨ ì‹œì‘!")
        print("=" * 50)
        
        try:
            # 1. ë°°ê²½ ì œê±°
            self.demo_background_removal(image_path)
            
            # 2. ê°ì²´ ì¶”ì¶œ
            self.demo_object_extraction(image_path, ['person', 'car', 'dog', 'cat'])
            
            # 3. ì–¼êµ´ ë·°í‹°
            self.demo_face_beauty(image_path)
            
            # 4. ê°ì²´ ì¹´ìš´íŒ…
            self.demo_object_counting(image_path)
            
            # 5. ì´ë¯¸ì§€ ë¶„ì„
            self.demo_image_analysis(image_path)
            
            # 6. ì´ë¯¸ì§€ í•„í„°
            self.demo_image_filter(image_path, 'blur', 'person')
            
            print("\nğŸ‰ ëª¨ë“  ì„œë¹„ìŠ¤ ë°ëª¨ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤ì€ 'results' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ ë°ëª¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    parser = argparse.ArgumentParser(description='ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„œë¹„ìŠ¤ ë°ëª¨')
    parser.add_argument('image_path', help='ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ')
    parser.add_argument('--service', choices=['all', 'bg_remove', 'extract', 'beauty', 'count', 'analyze', 'filter'], 
                       default='all', help='ì‹¤í–‰í•  ì„œë¹„ìŠ¤')
    parser.add_argument('--target_class', default='person', help='ëŒ€ìƒ ê°ì²´ í´ë˜ìŠ¤')
    parser.add_argument('--filter_type', choices=['blur', 'brightness', 'contrast'], 
                       default='blur', help='í•„í„° ì¢…ë¥˜')
    
    args = parser.parse_args()
    
    # ë°ëª¨ ì´ˆê¸°í™”
    demo = SegmentationServicesDemo()
    
    # ì„œë¹„ìŠ¤ ì‹¤í–‰
    if args.service == 'all':
        demo.run_all_demos(args.image_path)
    elif args.service == 'bg_remove':
        demo.demo_background_removal(args.image_path, args.target_class)
    elif args.service == 'extract':
        demo.demo_object_extraction(args.image_path, [args.target_class])
    elif args.service == 'beauty':
        demo.demo_face_beauty(args.image_path)
    elif args.service == 'count':
        demo.demo_object_counting(args.image_path)
    elif args.service == 'analyze':
        demo.demo_image_analysis(args.image_path)
    elif args.service == 'filter':
        demo.demo_image_filter(args.image_path, args.filter_type, args.target_class)

if __name__ == '__main__':
    main() 