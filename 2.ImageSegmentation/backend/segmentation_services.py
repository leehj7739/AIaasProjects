#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ í›„ ì œê³µí•  ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ë“¤
"""

import cv2
import numpy as np
from ultralytics import YOLO
import mediapipe as mp
from PIL import Image, ImageFilter, ImageEnhance
import requests
from pathlib import Path
import json
import base64
from io import BytesIO
from flask import Flask, request, jsonify, render_template, send_file, Response
import os
import time
from datetime import datetime

app = Flask(__name__)

class SegmentationServices:
    def __init__(self):
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.yolo_model = YOLO('yolov8n-seg.pt')
        
        # MediaPipe ì–¼êµ´ ëœë“œë§ˆí¬
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=True,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5
        )

    def detect_objects(self, image):
        """ê°ì²´ ê°ì§€ ë° ì„¸ê·¸ë©˜í…Œì´ì…˜"""
        results = self.yolo_model(image)
        return results

    def detect_face_landmarks(self, image):
        """ì–¼êµ´ ëœë“œë§ˆí¬ ê²€ì¶œ"""
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_image)
        return results

    # 1. ë°°ê²½ ì œê±°/êµì²´ ì„œë¹„ìŠ¤
    def remove_background(self, image, target_class='person'):
        """íŠ¹ì • ê°ì²´ì˜ ë°°ê²½ ì œê±°"""
        results = self.detect_objects(image)
        
        # ë§ˆìŠ¤í¬ ìƒì„±
        mask = np.zeros(image.shape[:2], dtype=np.uint8)
        
        for result in results:
            boxes = result.boxes
            masks = result.masks
            
            if boxes is not None and masks is not None:
                for i, box in enumerate(boxes):
                    cls = int(box.cls[0].cpu().numpy())
                    class_name = self.yolo_model.names[cls]
                    
                    if class_name == target_class:
                        mask_data = masks.data[i].cpu().numpy()
                        mask_resized = cv2.resize(mask_data, (image.shape[1], image.shape[0]))
                        mask = np.maximum(mask, (mask_resized > 0.5).astype(np.uint8) * 255)
        
        # ë°°ê²½ ì œê±°
        result_image = image.copy()
        result_image[mask == 0] = [255, 255, 255]  # í°ìƒ‰ ë°°ê²½
        
        return result_image, mask

    def replace_background(self, image, new_background_path, target_class='person'):
        """ë°°ê²½ êµì²´"""
        # ë°°ê²½ ì œê±°
        result_image, mask = self.remove_background(image, target_class)
        
        # ìƒˆ ë°°ê²½ ë¡œë“œ
        new_bg = cv2.imread(new_background_path)
        if new_bg is not None:
            new_bg = cv2.resize(new_bg, (image.shape[1], image.shape[0]))
            
            # ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ê²½ êµì²´
            mask_3d = np.stack([mask/255.0] * 3, axis=2)
            result_image = image * mask_3d + new_bg * (1 - mask_3d)
        
        return result_image.astype(np.uint8)

    # 2. ê°ì²´ ì¶”ì¶œ ë° ë¶„ë¦¬ ì„œë¹„ìŠ¤
    def extract_objects(self, image, target_classes=None):
        """íŠ¹ì • ê°ì²´ë“¤ ì¶”ì¶œ"""
        results = self.detect_objects(image)
        extracted_objects = []
        
        for result in results:
            boxes = result.boxes
            masks = result.masks
            
            if boxes is not None and masks is not None:
                for i, box in enumerate(boxes):
                    cls = int(box.cls[0].cpu().numpy())
                    class_name = self.yolo_model.names[cls]
                    
                    if target_classes is None or class_name in target_classes:
                        # ë§ˆìŠ¤í¬ ì ìš©
                        mask_data = masks.data[i].cpu().numpy()
                        mask_resized = cv2.resize(mask_data, (image.shape[1], image.shape[0]))
                        mask_bool = mask_resized > 0.5
                        
                        # ê°ì²´ ì¶”ì¶œ
                        extracted = image.copy()
                        extracted[~mask_bool] = [255, 255, 255]
                        
                        extracted_objects.append({
                            'class': class_name,
                            'image': extracted,
                            'mask': mask_bool,
                            'bbox': box.xyxy[0].cpu().numpy().tolist()
                        })
        
        return extracted_objects

    # 3. ì´ë¯¸ì§€ í¸ì§‘ ì„œë¹„ìŠ¤
    def apply_filter_to_object(self, image, target_class='person', filter_type='blur'):
        """íŠ¹ì • ê°ì²´ì— í•„í„° ì ìš©"""
        results = self.detect_objects(image)
        result_image = image.copy()
        
        for result in results:
            boxes = result.boxes
            masks = result.masks
            
            if boxes is not None and masks is not None:
                for i, box in enumerate(boxes):
                    cls = int(box.cls[0].cpu().numpy())
                    class_name = self.yolo_model.names[cls]
                    
                    if class_name == target_class:
                        mask_data = masks.data[i].cpu().numpy()
                        mask_resized = cv2.resize(mask_data, (image.shape[1], image.shape[0]))
                        mask_bool = mask_resized > 0.5
                        
                        # í•„í„° ì ìš©
                        if filter_type == 'blur':
                            blurred = cv2.GaussianBlur(result_image, (15, 15), 0)
                            result_image[mask_bool] = blurred[mask_bool]
                        elif filter_type == 'brightness':
                            hsv = cv2.cvtColor(result_image, cv2.COLOR_BGR2HSV)
                            hsv[mask_bool, 2] = np.clip(hsv[mask_bool, 2] * 1.5, 0, 255)
                            result_image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        
        return result_image

    # 4. ì–¼êµ´ ë·°í‹° ì„œë¹„ìŠ¤
    def apply_face_beauty(self, image):
        """ì–¼êµ´ ë·°í‹° í•„í„° ì ìš©"""
        results = self.detect_face_landmarks(image)
        result_image = image.copy()
        
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                h, w, _ = image.shape
                
                # ì–¼êµ´ ì˜ì—­ ë§ˆìŠ¤í¬ ìƒì„±
                face_mask = np.zeros((h, w), dtype=np.uint8)
                
                # ì–¼êµ´ ìœ¤ê³½ ëœë“œë§ˆí¬
                face_oval_indices = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,
                                   397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,
                                   172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67, 109, 10]
                
                face_points = []
                for idx in face_oval_indices:
                    if idx < len(face_landmarks.landmark):
                        landmark = face_landmarks.landmark[idx]
                        x = int(landmark.x * w)
                        y = int(landmark.y * h)
                        face_points.append([x, y])
                
                if len(face_points) > 2:
                    # ì–¼êµ´ ë§ˆìŠ¤í¬ ìƒì„±
                    face_points = np.array(face_points, dtype=np.int32)
                    cv2.fillPoly(face_mask, [face_points], 255)
                    
                    # ë·°í‹° í•„í„° ì ìš©
                    # 1. ìŠ¤ë¬´ë”© (ë¸”ëŸ¬)
                    smoothed = cv2.GaussianBlur(result_image, (5, 5), 0)
                    result_image = np.where(face_mask[:, :, np.newaxis] > 0, 
                                          smoothed, result_image)
                    
                    # 2. ë°ê¸° ì¡°ì •
                    hsv = cv2.cvtColor(result_image, cv2.COLOR_BGR2HSV)
                    hsv[face_mask > 0, 2] = np.clip(hsv[face_mask > 0, 2] * 1.1, 0, 255)
                    result_image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        
        return result_image

    # 5. ê°ì²´ ì¹´ìš´íŒ… ì„œë¹„ìŠ¤
    def count_objects(self, image, target_classes=None, draw_on_image=True):
        """ê°ì²´ ê°œìˆ˜ ì„¸ê¸°"""
        results = self.detect_objects(image)
        object_counts = {}
        result_image = image.copy() if draw_on_image else None
        
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    cls = int(box.cls[0].cpu().numpy())
                    class_name = self.yolo_model.names[cls]
                    
                    if target_classes is None or class_name in target_classes:
                        object_counts[class_name] = object_counts.get(class_name, 0) + 1
                        
                        # ì´ë¯¸ì§€ì— ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                        if draw_on_image:
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            conf = float(box.conf[0].cpu().numpy())
                            color = (0, 255, 0) if class_name == 'person' else (255, 0, 0)
                            label = f"{class_name} {conf:.2f}"
                            
                            cv2.rectangle(result_image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
                            cv2.putText(result_image, label, (int(x1), int(y1)-10), 
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        
        # ì‚¬ëŒ ê°ì§€ ê°œìˆ˜ë¥¼ ì¢Œí•˜ë‹¨ì— í‘œì‹œ
        if draw_on_image and 'person' in object_counts:
            person_count = object_counts['person']
            h, w = result_image.shape[:2]
            
            # ì‚¬ëŒ ê°ì§€ ê°œìˆ˜ í…ìŠ¤íŠ¸ (ì˜ì–´)
            text = f'People: {person_count}'
            
            # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 1.0
            thickness = 2
            (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)
            
            # ì¢Œí•˜ë‹¨ ìœ„ì¹˜ ê³„ì‚° (ì—¬ë°± 20í”½ì…€)
            text_x = 20
            text_y = h - 20
            
            # ë°°ê²½ ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
            cv2.rectangle(result_image, 
                         (text_x - 10, text_y - text_height - 10),
                         (text_x + text_width + 10, text_y + 10),
                         (0, 0, 0), -1)
            
            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            cv2.putText(result_image, text, (text_x, text_y), 
                       font, font_scale, (255, 255, 255), thickness)
        
        return object_counts, result_image if draw_on_image else object_counts

    # 6. ì´ë¯¸ì§€ ë¶„ì„ ì„œë¹„ìŠ¤
    def analyze_image(self, image):
        """ì´ë¯¸ì§€ ì¢…í•© ë¶„ì„"""
        results = self.detect_objects(image)
        analysis = {
            'objects': [],
            'total_objects': 0,
            'image_size': image.shape,
            'dominant_objects': []
        }
        
        object_counts = {}
        
        for result in results:
            boxes = result.boxes
            masks = result.masks
            
            if boxes is not None:
                for i, box in enumerate(boxes):
                    cls = int(box.cls[0].cpu().numpy())
                    class_name = self.yolo_model.names[cls]
                    conf = float(box.conf[0].cpu().numpy())
                    bbox = box.xyxy[0].cpu().numpy().tolist()
                    
                    # ê°ì²´ ì •ë³´
                    obj_info = {
                        'class': class_name,
                        'confidence': conf,
                        'bbox': bbox,
                        'area': (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
                    }
                    
                    # ë§ˆìŠ¤í¬ ì •ë³´ ì¶”ê°€
                    if masks is not None and i < len(masks.data):
                        mask_data = masks.data[i].cpu().numpy()
                        mask_resized = cv2.resize(mask_data, (image.shape[1], image.shape[0]))
                        mask_area = np.sum(mask_resized > 0.5)
                        obj_info['mask_area'] = mask_area
                    
                    analysis['objects'].append(obj_info)
                    object_counts[class_name] = object_counts.get(class_name, 0) + 1
        
        analysis['total_objects'] = len(analysis['objects'])
        analysis['object_counts'] = object_counts
        
        # ì£¼ìš” ê°ì²´ ì°¾ê¸° (ë©´ì  ê¸°ì¤€)
        if analysis['objects']:
            sorted_objects = sorted(analysis['objects'], key=lambda x: x.get('mask_area', 0), reverse=True)
            analysis['dominant_objects'] = [obj['class'] for obj in sorted_objects[:3]]
        
        return analysis

    # 7. ì´ë¯¸ì§€ ìƒì„± ì„œë¹„ìŠ¤
    def create_collage(self, image, layout='grid'):
        """ê°ì²´ë“¤ì„ ì´ìš©í•œ ì½œë¼ì£¼ ìƒì„±"""
        extracted_objects = self.extract_objects(image)
        
        if not extracted_objects:
            return image
        
        # ì½œë¼ì£¼ í¬ê¸° ê³„ì‚°
        n_objects = len(extracted_objects)
        if layout == 'grid':
            cols = int(np.ceil(np.sqrt(n_objects)))
            rows = int(np.ceil(n_objects / cols))
        else:
            cols = n_objects
            rows = 1
        
        # ì½œë¼ì£¼ ì´ë¯¸ì§€ ìƒì„±
        cell_size = 200
        collage = np.ones((rows * cell_size, cols * cell_size, 3), dtype=np.uint8) * 255
        
        for i, obj in enumerate(extracted_objects):
            row = i // cols
            col = i % cols
            
            # ê°ì²´ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
            obj_img = cv2.resize(obj['image'], (cell_size, cell_size))
            collage[row*cell_size:(row+1)*cell_size, col*cell_size:(col+1)*cell_size] = obj_img
        
        return collage

    # 8. ë°ì´í„° ì¶”ì¶œ ì„œë¹„ìŠ¤
    def extract_metadata(self, image):
        """ì´ë¯¸ì§€ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        analysis = self.analyze_image(image)
        
        metadata = {
            'filename': 'image.jpg',
            'size': f"{analysis['image_size'][1]}x{analysis['image_size'][0]}",
            'objects_detected': analysis['total_objects'],
            'object_types': list(analysis['object_counts'].keys()),
            'dominant_objects': analysis['dominant_objects'],
            'object_details': []
        }
        
        for obj in analysis['objects']:
            metadata['object_details'].append({
                'type': obj['class'],
                'confidence': f"{obj['confidence']:.2f}",
                'position': f"({obj['bbox'][0]:.0f}, {obj['bbox'][1]:.0f})",
                'size': f"{obj['bbox'][2]-obj['bbox'][0]:.0f}x{obj['bbox'][3]-obj['bbox'][1]:.0f}"
            })
        
        return metadata

# YOLO ëª¨ë¸ ë¡œë“œ (person í´ë˜ìŠ¤ë§Œ ì‚¬ìš©)
model = YOLO('yolov8n-seg.pt')  # í•„ìš”ì‹œ ê²½ë¡œ ìˆ˜ì •

UPLOADS_DIR = 'uploads'
RESULTS_DIR = 'results'
MAX_FILES = 30
os.makedirs(UPLOADS_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

def save_and_limit_dir(directory, filename, image):
    """ì´ë¯¸ì§€ ì €ì¥ ë° ìµœëŒ€ ê°œìˆ˜ ì œí•œ"""
    path = os.path.join(directory, filename)
    image.save(path)
    # íŒŒì¼ ê°œìˆ˜ ì œí•œ
    files = sorted([os.path.join(directory, f) for f in os.listdir(directory)], key=os.path.getctime)
    if len(files) > MAX_FILES:
        for f in files[:-MAX_FILES]:
            try:
                os.remove(f)
            except Exception as e:
                print(f"[íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜] {f}: {e}")
    return path

def log_with_time(msg):
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{now}] {msg}")

last_receive_time = None

@app.route('/detect_person', methods=['POST'])
def detect_person():
    global last_receive_time
    start_time = time.time()
    now = start_time
    # ìˆ˜ì‹  ê°„ê²© ê³„ì‚°
    if last_receive_time is not None:
        interval_ms = int((now - last_receive_time) * 1000)
        log_with_time(f"[detect_person] ì´ì „ ìš”ì²­ ëŒ€ë¹„ ìˆ˜ì‹  ê°„ê²©: {interval_ms} ms")
    last_receive_time = now
    if 'image' not in request.files:
        log_with_time("[detect_person] ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return jsonify({'error': 'No image uploaded'}), 400

    file = request.files['image']
    log_with_time(f"[detect_person] ì´ë¯¸ì§€ ìˆ˜ì‹ : {file.filename}")
    img = Image.open(file.stream).convert('RGB')

    # íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„+íŒŒì¼ëª…)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
    base_name = os.path.splitext(file.filename)[0]
    upload_filename = f"{timestamp}_{base_name}.jpg"
    result_filename = f"{timestamp}_{base_name}_result.jpg"

    # 1. ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥ (ìµœëŒ€ 30ê°œ)
    save_and_limit_dir(UPLOADS_DIR, upload_filename, img)

    # 2. YOLO ì¶”ë¡  ë° ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
    results = model(img)
    person_detected = False
    person_count = 0  # ì‚¬ëŒ ê°ì§€ ê°œìˆ˜
    # PIL -> numpy ë³€í™˜
    img_np = np.array(img)
    result_img_np = img_np.copy()
    for r in results:
        boxes = r.boxes
        if boxes is not None:
            for box in boxes:
                cls = int(box.cls[0].cpu().numpy())
                conf = float(box.conf[0].cpu().numpy())
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                color = (0,255,0) if cls == 0 else (255,0,0)
                label = f"{model.names[cls]} {conf:.2f}"
                # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                cv2.rectangle(result_img_np, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
                cv2.putText(result_img_np, label, (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                if cls == 0:
                    person_detected = True
                    person_count += 1
    
    # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ (ìµœëŒ€ 30ê°œ)
    result_img_pil = Image.fromarray(result_img_np)
    
    # ìš°í•˜ë‹¨ì— ì‚¬ëŒ ê°ì§€ ê°œìˆ˜ í‘œì‹œ
    h, w = result_img_np.shape[:2]
    if person_detected:
        # ì‚¬ëŒ ê°ì§€ ê°œìˆ˜ í…ìŠ¤íŠ¸ (ì˜ì–´)
        text = f'People: {person_count}'
        color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰ (BGR) - ì‚¬ëŒ ê°ì§€ ì‹œ
        
        # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 1.0
        thickness = 2
        (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)
        
        # ì¢Œí•˜ë‹¨ ìœ„ì¹˜ ê³„ì‚° (ì—¬ë°± 20í”½ì…€)
        text_x = 20
        text_y = h - 20
        
        # ë°°ê²½ ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
        cv2.rectangle(result_img_np, 
                     (text_x - 10, text_y - text_height - 10),
                     (text_x + text_width + 10, text_y + 10),
                     (0, 0, 0), -1)
        
        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
        cv2.putText(result_img_np, text, (text_x, text_y), 
                   font, font_scale, (255, 255, 255), thickness)
    else:
        # ì‚¬ëŒì´ ê°ì§€ë˜ì§€ ì•Šì€ ê²½ìš° (ì˜ì–´)
        text = 'No People'
        color = (0, 255, 0)  # ì´ˆë¡ìƒ‰ (BGR) - ì‚¬ëŒ ë¹„ê°ì§€ ì‹œ
        
        # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 1.0
        thickness = 2
        (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)
        
        # ì¢Œí•˜ë‹¨ ìœ„ì¹˜ ê³„ì‚° (ì—¬ë°± 20í”½ì…€)
        text_x = 20
        text_y = h - 20
        
        # ë°°ê²½ ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
        cv2.rectangle(result_img_np, 
                     (text_x - 10, text_y - text_height - 10),
                     (text_x + text_width + 10, text_y + 10),
                     (0, 0, 0), -1)
        
        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
        cv2.putText(result_img_np, text, (text_x, text_y), 
                   font, font_scale, (255, 255, 255), thickness)
    
    result_img_pil = Image.fromarray(result_img_np)
    save_and_limit_dir(RESULTS_DIR, result_filename, result_img_pil)

    status = "stop" if person_detected else "go"
    log_with_time(f"[detect_person] ê²°ê³¼: {status}, ì‚¬ëŒ ìˆ˜: {person_count}ëª…")
    end_time = time.time()
    elapsed_ms = int((end_time - start_time) * 1000)
    log_with_time(f"[detect_person] ì²˜ë¦¬~ì†¡ì‹ ê¹Œì§€ ì†Œìš” ì‹œê°„: {elapsed_ms} ms")
    return jsonify({'status': status, 'person_count': person_count})

@app.route('/health', methods=['GET'])
def health():
    log_with_time("[health] í—¬ìŠ¤ì²´í¬ ìš”ì²­ ìˆ˜ì‹ !")
    return jsonify({'status': 'ok'})

@app.route('/stream_results')
def stream_results():
    """results í´ë”ì˜ ì´ë¯¸ì§€ë¥¼ ì—°ì†ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ì›¹í˜ì´ì§€"""
    return render_template('results_stream.html')

@app.route('/api/latest_result_image')
def api_latest_result_image():
    """results í´ë”ì—ì„œ ê°€ì¥ ìµœê·¼ ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë°˜í™˜ (UDPì²˜ëŸ¼ ì‹ ë¢°ì„± ì—†ì´)"""
    try:
        files = [f for f in os.listdir(RESULTS_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        if not files:
            return jsonify({'success': False, 'error': 'No result images'}), 404
        files.sort(key=lambda x: os.path.getmtime(os.path.join(RESULTS_DIR, x)), reverse=True)
        latest_file = os.path.join(RESULTS_DIR, files[0])
        with open(latest_file, 'rb') as f:
            img_bytes = f.read()
        img_base64 = base64.b64encode(img_bytes).decode('utf-8')
        return jsonify({'success': True, 'image': img_base64, 'filename': files[0]})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/events')
def sse_events():
    def event_stream():
        last_filename = None
        while True:
            files = [f for f in os.listdir(RESULTS_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            if files:
                files.sort(key=lambda x: os.path.getmtime(os.path.join(RESULTS_DIR, x)), reverse=True)
                latest_file = files[0]
                if latest_file != last_filename:
                    last_filename = latest_file
                    yield f'data: {latest_file}\n\n'
            time.sleep(0.5)
    return Response(event_stream(), mimetype='text/event-stream')

def main():
    """ì„œë¹„ìŠ¤ ë°ëª¨"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„œë¹„ìŠ¤ ë°ëª¨')
    parser.add_argument('image_path', help='ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ')
    parser.add_argument('--service', choices=['remove_bg', 'extract', 'beauty', 'count', 'analyze', 'collage'], 
                       default='analyze', help='ì‚¬ìš©í•  ì„œë¹„ìŠ¤')
    parser.add_argument('--output', help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--show', action='store_true', help='ê²°ê³¼ í‘œì‹œ')
    
    args = parser.parse_args()
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    services = SegmentationServices()
    
    # ì´ë¯¸ì§€ ë¡œë“œ
    image = cv2.imread(args.image_path)
    if image is None:
        print(f"âŒ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.image_path}")
        return
    
    print(f"ğŸ”§ {args.service} ì„œë¹„ìŠ¤ ì‹¤í–‰ ì¤‘...")
    
    # ì„œë¹„ìŠ¤ ì‹¤í–‰
    if args.service == 'remove_bg':
        result, mask = services.remove_background(image)
        print("âœ… ë°°ê²½ ì œê±° ì™„ë£Œ")
        
    elif args.service == 'extract':
        objects = services.extract_objects(image, ['person', 'car', 'dog', 'cat'])
        print(f"âœ… {len(objects)}ê°œ ê°ì²´ ì¶”ì¶œ ì™„ë£Œ")
        result = services.create_collage(image)
        
    elif args.service == 'beauty':
        result = services.apply_face_beauty(image)
        print("âœ… ì–¼êµ´ ë·°í‹° í•„í„° ì ìš© ì™„ë£Œ")
        
    elif args.service == 'count':
        counts, result_image = services.count_objects(image)
        print("ğŸ“Š ê°ì²´ ê°œìˆ˜:")
        for obj, count in counts.items():
            print(f"  {obj}: {count}ê°œ")
        result = result_image
        
    elif args.service == 'analyze':
        analysis = services.analyze_image(image)
        print("ğŸ“Š ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:")
        print(f"  ì´ ê°ì²´ ìˆ˜: {analysis['total_objects']}")
        print(f"  ê°ì²´ ì¢…ë¥˜: {', '.join(analysis['object_counts'].keys())}")
        print(f"  ì£¼ìš” ê°ì²´: {', '.join(analysis['dominant_objects'])}")
        result = image
        
    elif args.service == 'collage':
        result = services.create_collage(image)
        print("âœ… ì½œë¼ì£¼ ìƒì„± ì™„ë£Œ")
    
    # ê²°ê³¼ ì €ì¥
    if args.output:
        cv2.imwrite(args.output, result)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {args.output}")
    else:
        output_path = f"result_{args.service}.jpg"
        cv2.imwrite(output_path, result)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ê²°ê³¼ í‘œì‹œ
    if args.show:
        cv2.imshow(f'{args.service} Result', result)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000) 